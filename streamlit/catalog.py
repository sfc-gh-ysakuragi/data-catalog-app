# æ—¢å­˜ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«è¿½åŠ 
import streamlit as st
from snowflake.snowpark.context import get_active_session
import plotly.express as px
import re
import ast
import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# ãƒšãƒ¼ã‚¸è¨­å®šï¼šå¹…åºƒãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚’ä½¿ç”¨
st.set_page_config(layout="wide")

# ç¾åœ¨ã®Snowflakeã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’å–å¾—
session = get_active_session()

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¸€è¦§ã‚’å–å¾—ã™ã‚‹é–¢æ•°ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãï¼‰
@st.cache_data
def get_databases():
    try:
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¸€è¦§ã‚’Snowflakeã‹ã‚‰å–å¾—
        databases = session.sql("""
            SELECT database_name
            FROM snowflake.account_usage.databases
            WHERE deleted IS NULL
            ORDER BY database_name
        """).toPandas()
        
        # '<Select>'ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’å…ˆé ­ã«è¿½åŠ 
        select_option = pd.DataFrame({'DATABASE_NAME': ['<Select>']})
        return pd.concat([select_option, databases], ignore_index=True)
    except Exception as e:
        st.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¸€è¦§ã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        return pd.DataFrame({'DATABASE_NAME': ['<Select>']})

# å…¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‚«ã‚¿ãƒ­ã‚°ã‚’å–å¾—ã™ã‚‹é–¢æ•°ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãï¼‰
@st.cache_data
def get_all_table_catalogs():
    try:
        all_catalogs = []
        # '<Select>'ã‚’é™¤å¤–ã—ãŸãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¸€è¦§ã‚’å–å¾—
        databases = session.sql("""
            SELECT database_name
            FROM snowflake.account_usage.databases
            WHERE deleted IS NULL
            ORDER BY database_name
        """).toPandas()
        
        for _, row in databases.iterrows():
            database_name = row['DATABASE_NAME']
            # æ—¢å­˜ã®get_table_catalogé–¢æ•°ã‚’ä½¿ç”¨
            catalog = get_table_catalog(database_name)
            if not catalog.empty:
                all_catalogs.append(catalog)
        
        if all_catalogs:
            return pd.concat(all_catalogs, ignore_index=True)
        return pd.DataFrame()
    except Exception as e:
        st.error(f"ãƒ†ãƒ¼ãƒ–ãƒ«ã‚«ã‚¿ãƒ­ã‚°ã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        return pd.DataFrame()

# å…¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®åˆ©ç”¨çµ±è¨ˆã‚’å–å¾—ã™ã‚‹é–¢æ•°ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãï¼‰
@st.cache_data
def get_all_usage_stats():
    try:
        all_stats = []
        # '<Select>'ã‚’é™¤å¤–ã—ãŸãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¸€è¦§ã‚’å–å¾—
        databases = session.sql("""
            SELECT database_name
            FROM snowflake.account_usage.databases
            WHERE deleted IS NULL
            ORDER BY database_name
        """).toPandas()
        
        for _, row in databases.iterrows():
            database_name = row['DATABASE_NAME']
            # æ—¢å­˜ã®get_table_usage_statsé–¢æ•°ã‚’ä½¿ç”¨
            stats = get_table_usage_stats(database_name)
            if not stats.empty:
                all_stats.append(stats)
        
        if all_stats:
            return pd.concat(all_stats, ignore_index=True)
        return pd.DataFrame()
    except Exception as e:
        st.error(f"åˆ©ç”¨çµ±è¨ˆã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        return pd.DataFrame()

# ãƒ†ãƒ¼ãƒ–ãƒ«ã‚«ã‚¿ãƒ­ã‚°ã‚’å–å¾—ã™ã‚‹é–¢æ•°
def get_table_catalog(databasename):
    try:
        df = session.sql(f"""
            SELECT DISTINCT 
                COMMENT, 
                table_catalog, 
                table_schema, 
                table_name, 
                table_owner, 
                row_count 
            FROM {databasename}.information_schema.tables 
            WHERE table_schema != 'INFORMATION_SCHEMA'
        """)
        return df.toPandas()
    except Exception as e:
        st.error(f"ãƒ†ãƒ¼ãƒ–ãƒ«ã‚«ã‚¿ãƒ­ã‚°ã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        return pd.DataFrame()

# ãƒ†ãƒ¼ãƒ–ãƒ«ã®çµ±è¨ˆæƒ…å ±ã‚’å–å¾—ã™ã‚‹é–¢æ•°ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãï¼‰
@st.cache_data()
def get_table_stats(database_name, schema_name, table_name):
    try:
        stats = session.sql(f"""
            SELECT 
                'Last Updated' as metric,
                TO_CHAR(MAX(LAST_ALTERED), 'YYYY-MM-DD HH24:MI:SS') as value
            FROM {database_name}.information_schema.tables 
            WHERE table_name = '{table_name}'
            AND table_schema = '{schema_name}'
            UNION ALL
            SELECT 
                'Created On',
                TO_CHAR(MIN(CREATED), 'YYYY-MM-DD HH24:MI:SS')
            FROM {database_name}.information_schema.tables 
            WHERE table_name = '{table_name}'
            AND table_schema = '{schema_name}'
            UNION ALL
            SELECT 
                'Storage Size (Bytes)',
                TO_CHAR(SUM(bytes), '999,999,999,999')
            FROM {database_name}.information_schema.tables 
            WHERE table_name = '{table_name}'
            AND table_schema = '{schema_name}'
        """)
        return stats.toPandas()
    except Exception as e:
        st.error(f"ãƒ†ãƒ¼ãƒ–ãƒ«çµ±è¨ˆã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        return pd.DataFrame()

# ãƒ†ãƒ¼ãƒ–ãƒ«ã®è¡Œæ•°ã‚’å–å¾—ã™ã‚‹é–¢æ•°ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãï¼‰
@st.cache_data()
def get_count(tablename):
    try:
        df = session.sql(f"SELECT COUNT(*) as count_rows FROM {tablename}")
        df = df.toPandas()
        count = df['COUNT_ROWS'].values[0]
        return format(count, ',')
    except Exception as e:
        st.error(f"è¡Œæ•°ã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        return "N/A"

# ã‚«ãƒ©ãƒ æƒ…å ±ã‚’å–å¾—ã™ã‚‹é–¢æ•°ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãï¼‰
@st.cache_data()
def get_column_data(tablename):
    df = session.sql("""
        select 
            TABLE_NAME, 
            COLUMN_NAME, 
            COMMENT 
        from information_schema.columns 
        where table_name = '""" + tablename + "'")
    return df.toPandas()


def get_response(session, prompt):
    # cortex.completeã¯roleãŒuserã§ãªã„ã¨å‹•ä½œã—ãªã„ã®ã§æ³¨æ„
    response = session.sql(f'''
    SELECT SNOWFLAKE.CORTEX.COMPLETE('claude-3-5-sonnet',
        {prompt},
        {{
            'temperature': 0,
            'top_p': 0
        }});
        ''').to_pandas().iloc[0,0]
    # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è¾æ›¸å‹ã«å¤‰æ›
    response = ast.literal_eval(response)
    response = response["choices"][0]["messages"]
    return response

def get_table_context(table_name, column_data):
    context = f"""
        ãƒ†ãƒ¼ãƒ–ãƒ«åã¯<tableName>"{str(table_name)}"</tableName>ã§ã™ã€‚
        SQLã®ã‚µãƒ³ãƒ—ãƒ«ã‚¯ã‚¨ãƒªã¯ã“ã¡ã‚‰ã§ã™ã€‚<ã‚µãƒ³ãƒ—ãƒ«ã‚¯ã‚¨ãƒª> select * from {str(table_name)} </ã‚µãƒ³ãƒ—ãƒ«ã‚¯ã‚¨ãƒª> 

        ã¾ãŸã€å¯¾è±¡ã®ãƒ†ãƒ¼ãƒ–ãƒ«ãŒæŒã¤åˆ—æƒ…å ±ã¯<columns>"{column_data}"</columns>ã§ã™ã€‚ 
    """
    return context

def get_system_prompt(table_name, column_data):
    table_context = get_table_context(table_name=table_name, column_data=column_data)
    return GEN_SQL.format(context=table_context)

GEN_SQL = """
ã‚ãªãŸã¯Snowflake SQL ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã¨ã—ã¦è¡Œå‹•ã—ã¾ã™ã€‚è³ªå•ã®å›ç­”ã¯æ—¥æœ¬èªã§ãŠé¡˜ã„ã—ã¾ã™ã€‚
ãƒ†ãƒ¼ãƒ–ãƒ«ãŒä¸ãˆã‚‰ã‚Œã‚‹ã®ã§ã€ãƒ†ãƒ¼ãƒ–ãƒ«åã¯ <tableName> ã‚¿ã‚°å†…ã«ã‚ã‚Šã€åˆ—ã¯ <columns> ã‚¿ã‚°å†…ã«ã‚ã‚‹ã®ã§ç¢ºèªã—ã¦ãã ã•ã„ã€‚
ãƒ†ãƒ¼ãƒ–ãƒ«ã®æ¦‚è¦ã¯ä»¥ä¸‹ã‚’å‚è€ƒã«ã—ã¦ãã ã•ã„ã€‚

{context}

ã“ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã®æ¦‚è¦ã‚’èª¬æ˜ã—ã€ã“ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã®å„è¡Œã«ã‚ã‚‹ãƒ‡ãƒ¼ã‚¿ã«ã©ã®ã‚ˆã†ãªç›¸é–¢ã‚„ç‰¹å¾´ãŒã‚ã‚‹ã‹ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚
ã¾ãŸåˆ—ã‚’ç¢ºèªã—åˆ©ç”¨å¯èƒ½ãªæŒ‡æ¨™ã‚’æ•°è¡Œã§å…±æœ‰ã—ã€ç®‡æ¡æ›¸ãã‚’ä½¿ç”¨ã—ã¦åˆ†æä¾‹ã‚’3ã¤ã‚’å¿…ãšæŒ™ã’ã¦ãã ã•ã„ã€‚
ã¾ãŸãªãœãã®åˆ†æä¾‹ãŒåŠ¹æœçš„ãªã®ã‹ã‚‚è©³ç´°ã«èª¬æ˜ã—ã€ã‚µãƒ³ãƒ—ãƒ«ã®SQLã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
"""

def get_cosine_similarity():
    """ãƒãƒ¼ã‚±ãƒƒãƒˆãƒ—ãƒ¬ã‚¤ã‚¹ã¨ã®é¡ä¼¼åº¦æ¤œç´¢"""
    search_results = session.sql(f"""
        SELECT 
            market.TITLE, 
            market.DESCRIPTION, 
            VECTOR_COSINE_SIMILARITY(catalog.embeddings, market.embeddings) as similarity
        FROM 
            DATA_CATALOG.TABLE_CATALOG.TABLE_CATALOG catalog, 
            DATA_CATALOG.TABLE_CATALOG.MARKETPLACE_EMBEDDING_LISTINGS market
        ORDER BY 
            similarity DESC
        LIMIT 10
        """).collect()
    return search_results


# ãƒ†ãƒ¼ãƒ–ãƒ«ã®åˆ©ç”¨çµ±è¨ˆã‚’å–å¾—ã™ã‚‹é–¢æ•°ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãï¼‰
@st.cache_data()
def get_table_usage_stats(database_name):
    """ãƒ†ãƒ¼ãƒ–ãƒ«ã®è©³ç´°ãªåˆ©ç”¨çµ±è¨ˆã‚’å–å¾—ã™ã‚‹é–¢æ•°"""
    try:
        usage_stats = session.sql(f"""
            WITH parsed_objects AS (
                SELECT 
                    query_id,
                    query_start_time,
                    f.value:objectDomain::STRING as obj_domain,
                    f.value:objectName::STRING as obj_name
                FROM SNOWFLAKE.ACCOUNT_USAGE.ACCESS_HISTORY,
                TABLE(FLATTEN(direct_objects_accessed)) f
                WHERE QUERY_START_TIME >= DATEADD(month, -3, CURRENT_TIMESTAMP())
                AND f.value:objectName::STRING LIKE '{database_name}.%'
            )
            SELECT 
                TO_VARCHAR(DATE(query_start_time)) as ACCESS_DATE,
                DAYNAME(query_start_time) as DAY_OF_WEEK,
                HOUR(query_start_time) as HOUR_OF_DAY,
                obj_name as TABLE_FULL_NAME,
                COUNT(DISTINCT query_id) as ACCESS_COUNT
            FROM parsed_objects
            WHERE obj_domain = 'Table'
            GROUP BY ACCESS_DATE, DAY_OF_WEEK, HOUR_OF_DAY, TABLE_FULL_NAME
            ORDER BY ACCESS_DATE
        """).toPandas()
        
        # åˆ—åã‚’å°æ–‡å­—ã«çµ±ä¸€
        usage_stats.columns = usage_stats.columns.str.lower()
        return usage_stats
    except Exception as e:
        st.error(f"ãƒ†ãƒ¼ãƒ–ãƒ«åˆ©ç”¨çµ±è¨ˆã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ç©ºã®DataFrameã‚’è¿”ã™
        return pd.DataFrame(columns=[
            'access_date', 'day_of_week', 'hour_of_day', 
            'table_full_name', 'access_count'
        ])

# åˆ©ç”¨çµ±è¨ˆã®å¯è¦–åŒ–ã‚’è¡Œã†é–¢æ•°
def display_usage_analytics(usage_stats, table_name=None):
    """åˆ©ç”¨çµ±è¨ˆã®å¯è¦–åŒ–ã‚’è¡Œã†é–¢æ•°"""
    if usage_stats.empty:
        st.warning("åˆ©ç”¨çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return
        
    # å¿…è¦ãªã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèª
    required_columns = ['access_date', 'day_of_week', 'hour_of_day', 'table_full_name', 'access_count']
    missing_columns = [col for col in required_columns if col not in usage_stats.columns]
    
    if missing_columns:
        st.error(f"å¿…è¦ãªã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {', '.join(missing_columns)}")
        return

    # ç‰¹å®šã®ãƒ†ãƒ¼ãƒ–ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    if table_name:
        usage_stats = usage_stats[usage_stats['table_full_name'] == table_name]

    # åˆ©ç”¨çµ±è¨ˆã®å¯è¦–åŒ–
    st.subheader("ğŸ“Š åˆ©ç”¨çµ±è¨ˆåˆ†æ")
    
    col1, col2 = st.columns(2)
    with col1:
        # æ—¥æ¬¡ã§ã®åˆ©ç”¨æ¨ç§»
        daily_usage = usage_stats.groupby('access_date')['access_count'].sum().reset_index()
        fig_daily = px.line(daily_usage, 
                           x='access_date', 
                           y='access_count',
                           title='æ—¥æ¬¡ã‚¢ã‚¯ã‚»ã‚¹æ¨ç§»',
                           labels={'access_date': 'æ—¥ä»˜', 'access_count': 'ã‚¢ã‚¯ã‚»ã‚¹æ•°'})
        st.plotly_chart(fig_daily, use_container_width=True)

    with col2:
        # æ™‚é–“å¸¯åˆ¥ã®åˆ©ç”¨å‚¾å‘
        hourly_usage = usage_stats.groupby('hour_of_day')['access_count'].sum().reset_index()
        fig_hourly = px.bar(hourly_usage, 
                           x='hour_of_day', 
                           y='access_count',
                           title='æ™‚é–“å¸¯åˆ¥ã‚¢ã‚¯ã‚»ã‚¹å‚¾å‘',
                           labels={'hour_of_day': 'æ™‚é–“', 'access_count': 'ã‚¢ã‚¯ã‚»ã‚¹æ•°'})
        st.plotly_chart(fig_hourly, use_container_width=True)

    # åˆ©ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ
    st.subheader("ğŸ“Š åˆ©ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ")
    col3, col4 = st.columns(2)
    
    with col3:
        # ã‚ˆãåˆ©ç”¨ã•ã‚Œã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°
        if not table_name:  # å…¨ä½“è¡¨ç¤ºã®å ´åˆã®ã¿è¡¨ç¤º
            table_ranking = usage_stats.groupby('table_full_name')['access_count'].sum() \
                                     .sort_values(ascending=False).head(10)
            fig_ranking = px.bar(table_ranking,
                                title='ã‚ˆãåˆ©ç”¨ã•ã‚Œã‚‹ãƒ†ãƒ¼ãƒ–ãƒ« TOP10',
                                labels={'table_full_name': 'ãƒ†ãƒ¼ãƒ–ãƒ«å', 'value': 'ã‚¢ã‚¯ã‚»ã‚¹æ•°'})
            st.plotly_chart(fig_ranking, use_container_width=True)

    with col4:
        # åˆ©ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æã¨ã‚¢ãƒ‰ãƒã‚¤ã‚¹
        peak_hour = hourly_usage.loc[hourly_usage['access_count'].idxmax()]
        st.info(f"ğŸ•’ æœ€ã‚‚ã‚¢ã‚¯ã‚»ã‚¹ãŒå¤šã„æ™‚é–“å¸¯: {int(peak_hour['hour_of_day'])}æ™‚ ({int(peak_hour['access_count'])}å›)")
        
        # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
        recent_trend = daily_usage.tail(7)['access_count'].mean()
        overall_trend = daily_usage['access_count'].mean()
        trend_diff = ((recent_trend - overall_trend) / overall_trend) * 100
        
        if abs(trend_diff) > 10:
            if trend_diff > 0:
                st.success(f"ğŸ“ˆ ç›´è¿‘1é€±é–“ã®ã‚¢ã‚¯ã‚»ã‚¹æ•°ãŒå¹³å‡ã‚ˆã‚Š{trend_diff:.1f}%å¢—åŠ ã—ã¦ã„ã¾ã™")
            else:
                st.warning(f"ğŸ“‰ ç›´è¿‘1é€±é–“ã®ã‚¢ã‚¯ã‚»ã‚¹æ•°ãŒå¹³å‡ã‚ˆã‚Š{abs(trend_diff):.1f}%æ¸›å°‘ã—ã¦ã„ã¾ã™")

# ãƒ‡ãƒ¼ã‚¿æ¢ç´¢ã®ãŸã‚ã®è³ªå•ã¨ã‚«ãƒ†ã‚´ãƒªã‚’ç”Ÿæˆ
@st.cache_data
def generate_discovery_questions():
    return [
        {"category": "å£²ä¸Šãƒ»è²¡å‹™", "question": "å£²ä¸Šã«é–¢ã™ã‚‹ãƒ‡ãƒ¼ã‚¿", "keywords": ["revenue", "sales", "income", "profit", "å£²ä¸Š", "åç›Š", "é‡‘é¡", "å˜ä¾¡"]},
        {"category": "é¡§å®¢", "question": "é¡§å®¢ãƒ‡ãƒ¼ã‚¿", "keywords": ["customer", "client", "user", "é¡§å®¢", "ãƒ¦ãƒ¼ã‚¶ãƒ¼", "ä¼šå“¡"]},
        {"category": "è£½å“", "question": "è£½å“æƒ…å ±", "keywords": ["product", "item", "inventory", "è£½å“", "å•†å“", "åœ¨åº«"]},
        {"category": "ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°", "question": "ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿", "keywords": ["marketing", "campaign", "advertisement", "åºƒå‘Š", "ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³"]},
        {"category": "å–å¼•", "question": "å–å¼•ãƒ‡ãƒ¼ã‚¿", "keywords": ["transaction", "payment", "order", "å–å¼•", "æ³¨æ–‡", "æ”¯æ‰•"]},
        {"category": "æ™‚ç³»åˆ—", "question": "æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿", "keywords": ["daily", "monthly", "yearly", "æ—¥æ¬¡", "æœˆæ¬¡", "å¹´æ¬¡", "æ¨ç§»"]},
        {"category": "åœ°åŸŸ", "question": "åœ°åŸŸåˆ¥ã®ãƒ‡ãƒ¼ã‚¿", "keywords": ["region", "area", "location", "åœ°åŸŸ", "éƒ½é“åºœçœŒ", "å¸‚åŒºç”ºæ‘"]},
        {"category": "çµ„ç¹”", "question": "çµ„ç¹”ãƒ»éƒ¨é–€åˆ¥ã®ãƒ‡ãƒ¼ã‚¿", "keywords": ["department", "division", "organization", "éƒ¨é–€", "çµ„ç¹”", "éƒ¨ç½²"]},
    ]



# ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«åŸºã¥ã„ã¦ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
def filter_tables_by_keywords(table_catalog, keywords):
    filtered_tables = []
    for _, row in table_catalog.iterrows():
        comment = str(row['COMMENT']).lower() if pd.notna(row['COMMENT']) else ""
        table_name = str(row['TABLE_NAME']).lower()
        
        if any(keyword.lower() in comment or keyword.lower() in table_name for keyword in keywords):
            filtered_tables.append(row)
    
    return pd.DataFrame(filtered_tables)

# äººæ°—ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å–å¾—
def get_popular_tables(usage_stats, table_catalog, limit=5):
    if usage_stats.empty:
        return pd.DataFrame()
    
    popular_tables = usage_stats.groupby('table_full_name')['access_count'].sum() \
                              .sort_values(ascending=False) \
                              .head(limit)
    return popular_tables

# ãŠã™ã™ã‚ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’è¡¨ç¤º
def display_recommended_tables(table_catalog, usage_stats):
    if table_catalog.empty or usage_stats.empty:
        st.info("ãƒ†ãƒ¼ãƒ–ãƒ«ã®åˆ©ç”¨çµ±è¨ˆæƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“")
        return
        
    st.markdown("### ğŸ’¡ ãŠã™ã™ã‚ã®ãƒ†ãƒ¼ãƒ–ãƒ«")
    
    # äººæ°—ã®ãƒ†ãƒ¼ãƒ–ãƒ«
    popular_tables = get_popular_tables(usage_stats, table_catalog)
    if not popular_tables.empty:
        st.markdown("#### ğŸ‘¥ ã‚ˆãä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«")
        for table_name, access_count in popular_tables.items():
            # ãƒ†ãƒ¼ãƒ–ãƒ«åã‹ã‚‰å„éƒ¨åˆ†ã‚’æŠ½å‡º
            table_parts = table_name.split('.')
            if len(table_parts) >= 3:
                catalog = table_parts[0]
                schema = table_parts[1]
                name = table_parts[2]
                
                # ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã‚’æ¤œç´¢
                table_match = table_catalog[
                    (table_catalog['TABLE_CATALOG'] == catalog) &
                    (table_catalog['TABLE_SCHEMA'] == schema) &
                    (table_catalog['TABLE_NAME'] == name)
                ]
                
                if not table_match.empty:
                    table_info = table_match.iloc[0]
                    with st.expander(f"**{name}** (ã‚¢ã‚¯ã‚»ã‚¹æ•°: {access_count})", expanded=False):
                        # ãƒ†ãƒ¼ãƒ–ãƒ«ã®èª¬æ˜æ–‡ãŒã‚ã‚‹å ´åˆã¯è¡¨ç¤º
                        if pd.notna(table_info['COMMENT']):
                            st.write(table_info['COMMENT'])
                        else:
                            st.write("èª¬æ˜æ–‡ã¯ã‚ã‚Šã¾ã›ã‚“")
                            
                        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®è¡¨ç¤º
                        col1, col2 = st.columns(2)
                        with col1:
                            st.metric("ã‚¹ã‚­ãƒ¼ãƒ", table_info['TABLE_SCHEMA'])
                        with col2:
                            if pd.notna(table_info['ROW_COUNT']):
                                st.metric("è¡Œæ•°", format(table_info['ROW_COUNT'], ','))
                        
                        # è©³ç´°ãƒœã‚¿ãƒ³
                        if st.button("è©³ç´°ã‚’è¦‹ã‚‹", key=f"popular_{table_name}"):
                            st.session_state.selected_table = table_name
                else:
                    st.warning(f"ãƒ†ãƒ¼ãƒ–ãƒ« {name} ã®è©³ç´°æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            else:
                st.warning(f"ãƒ†ãƒ¼ãƒ–ãƒ«å {table_name} ã®å½¢å¼ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“")

# æ–°ã—ã„é–¢æ•°: ãƒ†ãƒ¼ãƒ–ãƒ«æ¤œç´¢æ©Ÿèƒ½
def search_tables(table_catalog, search_term):
    if not search_term:
        return table_catalog
    
    search_term = search_term.lower()
    filtered = table_catalog[
        table_catalog['TABLE_NAME'].str.lower().str.contains(search_term) |
        table_catalog['COMMENT'].str.lower().str.contains(search_term, na=False)
    ]
    return filtered

# ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
st.title("Snowflake ãƒ‡ãƒ¼ã‚¿ã‚«ã‚¿ãƒ­ã‚° â„ï¸")
st.subheader(f"ã‚ˆã†ã“ã  :blue[{str(st.experimental_user.user_name)}] ã•ã‚“")

# ã‚¿ãƒ–ã®ä½œæˆ
tab1, tab2 = st.tabs(["ğŸ“Š ç‰¹å®šã® DB ã‹ã‚‰è©³ç´°ã‚’åˆ†æ", "ğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’æ¢ã™"])

with tab1:
    # æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹é¸æŠUI
    df_databases = get_databases()
    filter_database = st.selectbox('ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’é¸æŠã—ã¦ãã ã•ã„', df_databases['DATABASE_NAME'])
    
    if not '<Select>' in filter_database:
        database_name = filter_database.split(' ')[0].replace('(','').replace(')','')
        
        with st.spinner('ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æä¸­'):
            # æ—¢å­˜ã®ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§è¡¨ç¤ºã‚³ãƒ¼ãƒ‰...
            # (ä»¥ä¸‹ã€æ—¢å­˜ã®ã‚³ãƒ¼ãƒ‰ã‚’ãã®ã¾ã¾ç¶­æŒ)
            table_catalog = get_table_catalog(database_name)
            usage_stats = get_table_usage_stats(database_name)

            # å…¨ä½“ã®åˆ©ç”¨çµ±è¨ˆã‚’è¡¨ç¤º
            with st.expander("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å…¨ä½“ã®åˆ©ç”¨çµ±è¨ˆ", expanded=False):
                display_usage_analytics(usage_stats)
            
            # 4åˆ—ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã§ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’è¡¨ç¤º
            st.header("ğŸ“‘ ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§")
            col1, col2, col3, col4 = st.columns(4)
            
            for index, row in table_catalog.iterrows():
                current_col = [col1, col2, col3, col4][index % 4]
                with current_col:
                    with st.expander("**"+row['TABLE_NAME']+"**", expanded=True):
                        full_table_name = f"{row['TABLE_CATALOG']}.{row['TABLE_SCHEMA']}.{row['TABLE_NAME']}"
                        
                        st.write(row['COMMENT'])
                        
                        if not usage_stats.empty:
                            table_access = usage_stats[
                                usage_stats['table_full_name'] == full_table_name
                            ]['access_count'].sum()
                            
                            st.markdown(
                                f"""
                                <div style='
                                    background-color: #eef1f6;
                                    padding: 8px 15px;
                                    border-radius: 5px;
                                    margin: 10px 0;
                                    display: inline-block;
                                    border: 1px solid #e0e4eb;
                                '>
                                    <span style='font-size: 0.9em; color: #666;'>ğŸ‘¥ éå»3ãƒ¶æœˆã®ã‚¢ã‚¯ã‚»ã‚¹æ•°:</span>
                                    <span style='font-size: 1.1em; font-weight: bold; margin-left: 8px; color: #2c3e50;'>{table_access}</span>
                                </div>
                                """,
                                unsafe_allow_html=True
                            )
                        
                        key_details = full_table_name
                        get_data_details = st.button("è©³ç´°", key=key_details, type="primary")
                        
                # è©³ç´°æƒ…å ±ã®è¡¨ç¤º
                if get_data_details:
                    st.session_state.messages = []
                    
                    count_rows = get_count(key_details)
                    table_parts = key_details.split('.')
                    database_name = table_parts[0]
                    schema_name = table_parts[1]
                    table_name = table_parts[2]

                    with st.expander(str(key_details) + " ã®æ¦‚è¦", expanded=True):
                        st.success("ãƒ¬ã‚³ãƒ¼ãƒ‰æ•° : " + str(count_rows))

                        st.info("ğŸ“Š ãƒ†ãƒ¼ãƒ–ãƒ«çµ±è¨ˆæƒ…å ±")
                        stats_df = get_table_stats(database_name, schema_name, table_name)
                        if not stats_df.empty:
                            st.dataframe(stats_df, use_container_width=True)

                        st.info("ãƒ†ãƒ¼ãƒ–ãƒ«å†…ã®ã‚«ãƒ©ãƒ åã¨èª¬æ˜")
                        sql = session.sql(f"select * from {key_details} limit 10")
                        st.dataframe(sql, use_container_width=True)

                    with st.expander("LLMã‚’ä½¿ã£ãŸãƒ†ãƒ¼ãƒ–ãƒ«ã®è©³ç´°åˆ†æ"):
                        column_data = get_column_data(table_name)
                        prompt = get_system_prompt(table_name, column_data)
                        st.session_state.messages.append({"role": 'user', "content": prompt})

                        response = get_response(session, st.session_state.messages)
                        st.session_state.messages.append({"role": "assistant", "content": response})
                        st.markdown(response)

                    with st.expander("ãƒãƒ¼ã‚±ãƒƒãƒˆãƒ—ãƒ¬ã‚¤ã‚¹ã§å½¹ç«‹ã¡ãã†ãªãƒ‡ãƒ¼ã‚¿ä¸Šä½10ä»¶"):
                        results = get_cosine_similarity()
                        st.dataframe(results, use_container_width=True)

with tab2:
    st.markdown("### ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’æ¢ã™")
    
    # æ¤œç´¢ã®ãƒ’ãƒ³ãƒˆè¡¨ç¤º
    with st.expander("æ¤œç´¢ã®ãƒ’ãƒ³ãƒˆ", expanded=False):
        st.markdown("""
        #### åŠ¹æœçš„ãªæ¤œç´¢ã®ã‚³ãƒ„
        - **å…·ä½“çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰**: ã€Œå£²ä¸Šã€ã€Œé¡§å®¢ã€ãªã©å…·ä½“çš„ãªå˜èªã§æ¤œç´¢
        - **è¤‡æ•°ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰**: ã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šã§è¤‡æ•°ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›å¯èƒ½
        - **æ—¥æœ¬èª/è‹±èª**: æ—¥æœ¬èªã¨è‹±èªã©ã¡ã‚‰ã§ã‚‚æ¤œç´¢å¯èƒ½
        
        #### ã‚ˆãä½¿ã‚ã‚Œã‚‹æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¾‹
        - å£²ä¸Šé–¢é€£: revenue, sales, å£²ä¸Š, åç›Š
        - é¡§å®¢é–¢é€£: customer, user, é¡§å®¢, ãƒ¦ãƒ¼ã‚¶ãƒ¼
        - è£½å“é–¢é€£: product, item, è£½å“, å•†å“
        - æœŸé–“é–¢é€£: daily, monthly, æ—¥æ¬¡, æœˆæ¬¡
        """)
    
    # æ¤œç´¢ãƒãƒ¼
    search_term = st.text_input("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§æ¤œç´¢", placeholder="ãƒ†ãƒ¼ãƒ–ãƒ«åã‚„èª¬æ˜æ–‡ã§æ¤œç´¢...")
    
    # ãƒ‡ãƒ¼ã‚¿æ¢ç´¢ã®ãŸã‚ã®è³ªå•
    st.markdown("### ã©ã‚“ãªãƒ‡ãƒ¼ã‚¿ã‚’ãŠæ¢ã—ã§ã™ã‹ï¼Ÿ")
    questions = generate_discovery_questions()
    selected_purposes = []
    
    cols = st.columns(3)
    for i, q in enumerate(questions):
        with cols[i % 3]:
            if st.checkbox(q["question"]):
                selected_purposes.extend(q["keywords"])
    
    with st.spinner('ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æä¸­...'):
        # å…¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã‚’å–å¾—
        table_catalog = get_all_table_catalogs()
        usage_stats = get_all_usage_stats()
        
        # æ¤œç´¢çµæœã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°çµæœã®çµ±åˆ
        filtered_catalog = table_catalog
        if search_term:
            filtered_catalog = search_tables(filtered_catalog, search_term)
        if selected_purposes:
            filtered_catalog = filter_tables_by_keywords(filtered_catalog, selected_purposes)
        
        if len(filtered_catalog) > 0:
            st.markdown(f"### æ¤œç´¢çµæœ: {len(filtered_catalog)}ä»¶ã®ãƒ†ãƒ¼ãƒ–ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
            
            # ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§ã®è¡¨ç¤º
            for _, row in filtered_catalog.iterrows():
                with st.expander(f"**{row['TABLE_CATALOG']}.{row['TABLE_SCHEMA']}.{row['TABLE_NAME']}**", expanded=False):
                    st.write(row['COMMENT'])
                    full_table_name = f"{row['TABLE_CATALOG']}.{row['TABLE_SCHEMA']}.{row['TABLE_NAME']}"
                    
                    if not usage_stats.empty:
                        table_access = usage_stats[
                            usage_stats['table_full_name'] == full_table_name
                        ]['access_count'].sum()
                        st.metric("ğŸ‘¥ éå»3ãƒ¶æœˆã®ã‚¢ã‚¯ã‚»ã‚¹æ•°", table_access)
                    
        else:
            st.info("æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        
        # ãŠã™ã™ã‚ã®ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤º
        if not search_term and not selected_purposes:
            display_recommended_tables(table_catalog, usage_stats)